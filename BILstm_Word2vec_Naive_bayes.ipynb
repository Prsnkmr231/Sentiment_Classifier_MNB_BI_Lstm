{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f77e4195-3245-4e68-9ebc-e8d994b9a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text  import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Dense,Bidirectional,LSTM\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text  import Tokenizer,one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d2bddd-7f35-43dd-9c41-66ce026fb922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running index is 0\n",
      "running index is 1000\n",
      "running index is 2000\n",
      "running index is 3000\n",
      "running index is 4000\n",
      "running index is 5000\n",
      "running index is 6000\n",
      "running index is 7000\n",
      "running index is 8000\n",
      "running index is 9000\n",
      "running index is 10000\n",
      "running index is 11000\n",
      "running index is 12000\n",
      "running index is 13000\n",
      "running index is 14000\n",
      "running index is 15000\n",
      "running index is 16000\n",
      "running index is 17000\n",
      "running index is 18000\n",
      "running index is 19000\n",
      "running index is 20000\n",
      "running index is 21000\n",
      "running index is 22000\n",
      "running index is 23000\n",
      "running index is 24000\n",
      "running index is 25000\n",
      "running index is 26000\n",
      "running index is 27000\n",
      "running index is 28000\n",
      "running index is 29000\n",
      "running index is 30000\n",
      "running index is 31000\n",
      "running index is 32000\n",
      "running index is 33000\n",
      "running index is 34000\n",
      "running index is 35000\n",
      "running index is 36000\n",
      "running index is 37000\n",
      "running index is 38000\n",
      "running index is 39000\n",
      "running index is 40000\n",
      "running index is 41000\n",
      "running index is 42000\n",
      "running index is 43000\n",
      "running index is 44000\n",
      "running index is 45000\n",
      "running index is 46000\n",
      "running index is 47000\n",
      "running index is 48000\n",
      "running index is 49000\n"
     ]
    }
   ],
   "source": [
    "messages = pd.read_csv(\"imdb_dataset.csv\")\n",
    "\n",
    "# messages = messages.iloc[:20,:]\n",
    "\n",
    "corpus = []\n",
    "WL =  WordNetLemmatizer()\n",
    "for index in range(len(messages)):\n",
    "  if index%1000==0:\n",
    "     print(f\"running index is {index}\")\n",
    "  html_tag_remover = re.sub(\"<.*>\",\" \",messages[\"review\"][index])\n",
    "  filtered_words = re.sub(\"\\W\",\" \",html_tag_remover)\n",
    "  message = filtered_words.lower()\n",
    "  words_lst = [WL.lemmatize(word) for word in nltk.word_tokenize(message) if word not in stopwords.words('english')]\n",
    "#   words_lst = [PS.stem(word) for word in nltk.word_tokenize(message) if word not in stopwords.words('english')]\n",
    "  document = \" \".join(words_lst)\n",
    "  corpus.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7a0f56-7dc1-460c-9e6c-a0aec562016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "order =  [\"negative\",\"positive\"]\n",
    "le.fit(order)\n",
    "Y=le.transform(messages['sentiment'])\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(corpus,Y,test_size=0.2,random_state=4)\n",
    "# ##Using onehot vector representation for converting the sequences into vectors.\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)+1\n",
    "sent_length = 100\n",
    "emb_dim = 100\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences,padding=\"post\",maxlen=sent_length)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences,padding=\"post\",maxlen=sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c48ebd-5e18-4e36-93e5-427608821b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 119s 94ms/step - loss: 0.3927 - accuracy: 0.8204\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 113s 91ms/step - loss: 0.2052 - accuracy: 0.9201\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 114s 91ms/step - loss: 0.1184 - accuracy: 0.9559\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 119s 95ms/step - loss: 0.0683 - accuracy: 0.9752\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 0.0397 - accuracy: 0.9865\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 118s 94ms/step - loss: 0.0346 - accuracy: 0.9883\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 116s 93ms/step - loss: 0.0216 - accuracy: 0.9926\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 116s 93ms/step - loss: 0.0164 - accuracy: 0.9948\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 117s 94ms/step - loss: 0.0165 - accuracy: 0.9944\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 117s 93ms/step - loss: 0.0124 - accuracy: 0.9963\n",
      "313/313 [==============================] - 5s 15ms/step\n",
      "[[3.3430302e-07]\n",
      " [9.9961942e-01]\n",
      " [9.3474811e-01]\n",
      " [1.3406038e-03]\n",
      " [1.4102313e-04]\n",
      " [9.7864622e-01]\n",
      " [9.9997783e-01]\n",
      " [1.6414369e-02]\n",
      " [9.9575210e-01]\n",
      " [9.9973077e-01]]\n",
      "0.8338\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classfication_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m         value[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(Y_test,prediction))\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassfication_report\u001b[49m(Y_test,prediction))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classfication_report' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=emb_dim,input_length=sent_length))\n",
    "model.add(Bidirectional(LSTM(units=128)))\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(32,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.fit(train_padded,Y_train,epochs=10,verbose=1)\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "print(prediction[:10])\n",
    "for value in prediction:\n",
    "    if value[0]>0.5:\n",
    "        value[0]=1\n",
    "    else:\n",
    "        value[0]=0\n",
    "print(accuracy_score(Y_test,prediction))\n",
    "print(classification_report(Y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8f33db-8a37-4f2e-872a-4b19b86be6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 13ms/step\n",
      "[[3.3430302e-07]\n",
      " [9.9961942e-01]\n",
      " [9.3474811e-01]\n",
      " [1.3406038e-03]\n",
      " [1.4102313e-04]\n",
      " [9.7864622e-01]\n",
      " [9.9997783e-01]\n",
      " [1.6414369e-02]\n",
      " [9.9575210e-01]\n",
      " [9.9973077e-01]]\n",
      "0.8338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.82      5036\n",
      "           1       0.80      0.89      0.84      4964\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.83      0.83     10000\n",
      "weighted avg       0.84      0.83      0.83     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "\n",
    "print(prediction[:10])\n",
    "for value in prediction:\n",
    "    if value[0]>0.5:\n",
    "        value[0]=1\n",
    "    else:\n",
    "        value[0]=0\n",
    "print(accuracy_score(Y_test,prediction))\n",
    "print(classification_report(Y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "230d7629-dc6e-40ba-91a4-149ae08ad033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running index is 0\n",
      "running index is 1000\n",
      "running index is 2000\n",
      "running index is 3000\n",
      "running index is 4000\n",
      "running index is 5000\n",
      "running index is 6000\n",
      "running index is 7000\n",
      "running index is 8000\n",
      "running index is 9000\n",
      "running index is 10000\n",
      "running index is 11000\n",
      "running index is 12000\n",
      "running index is 13000\n",
      "running index is 14000\n",
      "running index is 15000\n",
      "running index is 16000\n",
      "running index is 17000\n",
      "running index is 18000\n",
      "running index is 19000\n",
      "running index is 20000\n",
      "running index is 21000\n",
      "running index is 22000\n",
      "running index is 23000\n",
      "running index is 24000\n",
      "running index is 25000\n",
      "running index is 26000\n",
      "running index is 27000\n",
      "running index is 28000\n",
      "running index is 29000\n",
      "running index is 30000\n",
      "running index is 31000\n",
      "running index is 32000\n",
      "running index is 33000\n",
      "running index is 34000\n",
      "running index is 35000\n",
      "running index is 36000\n",
      "running index is 37000\n",
      "running index is 38000\n",
      "running index is 39000\n",
      "running index is 40000\n",
      "running index is 41000\n",
      "running index is 42000\n",
      "running index is 43000\n",
      "running index is 44000\n",
      "running index is 45000\n",
      "running index is 46000\n",
      "running index is 47000\n",
      "running index is 48000\n",
      "running index is 49000\n"
     ]
    }
   ],
   "source": [
    "messages = pd.read_csv(\"imdb_dataset.csv\")\n",
    "\n",
    "# messages = messages.iloc[:300,:]\n",
    "corpus_words = []\n",
    "corpus_sentences = []\n",
    "WL =  WordNetLemmatizer()\n",
    "for index in range(len(messages)):\n",
    "  if index%1000==0:\n",
    "     print(f\"running index is {index}\")\n",
    "  html_tag_remover = re.sub(\"<.*>\",\" \",messages[\"review\"][index])\n",
    "  filtered_words = re.sub(\"\\W\",\" \",html_tag_remover)\n",
    "  message = filtered_words.lower()\n",
    "  words_lst = [WL.lemmatize(word) for word in nltk.word_tokenize(message) if word not in stopwords.words('english')]\n",
    "  document = \" \".join(words_lst)\n",
    "  corpus_sentences.append(document)\n",
    "  corpus_words.append(words_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "696caf61-f64c-444a-ac73-91c607ffbb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73253\n",
      "number of key-value pairs in dict is 73253\n",
      "first padded sentence is [    3   958  1076    51   177  2546   155  2568   109   529   473     8\n",
      "    41   216   966    20   593   101    33    20  2420   685    91   230\n",
      "  4020  2055   179   685  1119   685   692  2546   724   115    21   155\n",
      "    35    80  3158  1453  2221    41  1383   139  1327   809  2546    86\n",
      "  9200   188   443  1199   481   481  6080  6562  2331  2639 16915  4779\n",
      "   316   497    14   156    15  7392   572   523  4779   494  1094  4936\n",
      "   593   365   605  1241  1094   359    51  2546   108   356  3873  3304\n",
      "   482  1188    14   822  4104   355     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0]\n",
      "first element in embed_matrix is:1.1139309406280518\n",
      "vocab_size is: 73157\n",
      "embed matrix shape is : (73157, 300)\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_18 (Embedding)    (None, 553, 300)          21947100  \n",
      "                                                                 \n",
      " bidirectional_16 (Bidirect  (None, 256)               439296    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22402909 (85.46 MB)\n",
      "Trainable params: 22402909 (85.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "1250/1250 [==============================] - 621s 496ms/step - loss: 0.4006 - accuracy: 0.8209\n",
      "Epoch 2/3\n",
      "1250/1250 [==============================] - 617s 494ms/step - loss: 0.2585 - accuracy: 0.8960\n",
      "Epoch 3/3\n",
      "1250/1250 [==============================] - 617s 493ms/step - loss: 0.1560 - accuracy: 0.9426\n",
      "313/313 [==============================] - 33s 104ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "[1]\n",
      "0.8656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86      4961\n",
      "           1       0.87      0.87      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from keras.initializers import Constant\n",
    "embed_dim =300\n",
    "w2v_model=Word2Vec(sentences=corpus_words,vector_size=embed_dim,window=10,min_count=1)\n",
    "w2v_model.train(corpus_words,epochs=10,total_examples=len(corpus_words))\n",
    "vocab = w2v_model.wv.key_to_index\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "vocab=list(vocab.keys())\n",
    "word_vec_dict={}\n",
    "for word in vocab:\n",
    "  word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
    "\n",
    "print(f\"number of key-value pairs in dict is {len(word_vec_dict)}\")\n",
    "\n",
    "max_num_tokens = -1\n",
    "for sentence in corpus_words:\n",
    "    if len(sentence)>max_num_tokens:\n",
    "        max_num_tokens = len(sentence)\n",
    "\n",
    "\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus_sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "tokenized_sequences = tokenizer.texts_to_sequences(corpus_sentences)\n",
    "padded_sentences= pad_sequences(tokenized_sequences, maxlen=max_num_tokens, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "print(f\"first padded sentence is {padded_sentences[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "  if word in word_vec_dict.keys():\n",
    "      embed_vector=word_vec_dict[word]\n",
    "      if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "        embed_matrix[i]=embed_vector\n",
    "\n",
    "\n",
    "print(f\"first element in embed_matrix is:{embed_matrix[10][10]}\")\n",
    "print(\"vocab_size is:\",vocab_size)\n",
    "print(\"embed matrix shape is :\",embed_matrix.shape)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "order =  [\"negative\",\"positive\"]\n",
    "le.fit(order)\n",
    "Y=le.transform(messages['sentiment'])\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(padded_sentences,Y,test_size=0.20,random_state=42)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_num_tokens,embeddings_initializer=Constant(embed_matrix)))\n",
    "model.add(Bidirectional(LSTM(units=128)))\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(x_train,y_train,epochs=3,verbose=1)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "pred = (pred > 0.5).astype(int)\n",
    "\n",
    "print(type(pred))\n",
    "print(pred[0])\n",
    "\n",
    "print(accuracy_score(y_test,pred))\n",
    "# print(accuracy_score(y_test,prediction))\n",
    "# print(f\"accuracy_score:{accuracy}\")\n",
    "# print(f\"classification_report:{classification_report}\")\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1a7da-870f-48f5-86c4-bf8e76dc33b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
